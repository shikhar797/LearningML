{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cni4Wi1P6tOZ"
      },
      "source": [
        "## **Stemming Exercise**\n",
        "\n",
        "An online job search platform allows users to search for job listings using free-text queries. The platform stores thousands of job descriptions and user queries in English. However, the search engine often fails to return relevant results because different grammatical forms of the same word are treated as separate terms.\n",
        "\n",
        "For example, job descriptions may contain words like “developing”, “developer”, and “development”, while users may search for “develop”. This mismatch reduces the accuracy of search results.\n",
        "\n",
        "To improve search relevance and reduce vocabulary size, the platform plans to apply stemming as part of its text preprocessing pipeline.\n",
        "\n",
        "## **Problem Statement**\n",
        "\n",
        "You are part of a data science team responsible for enhancing the search functionality of the job portal. Your task is to analyze how stemming can help normalize textual data and compare the behavior of different stemming techniques.\n",
        "\n",
        "You are given a dataset containing:\n",
        "\n",
        "1. Job descriptions\n",
        "\n",
        "2. User search queries\n",
        "\n",
        "Before applying any machine learning or information retrieval techniques, the text must be preprocessed using stemming algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkvfJSEd7gHe",
        "outputId": "38822c24-cff0-4e95-8ed0-7c4eecbc014b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download required NLTK Resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xEIXHac7mGo"
      },
      "outputs": [],
      "source": [
        "# Job Descriptions and user queries\n",
        "\n",
        "job_descriptions = [\n",
        "    \"We are looking for a software developer who enjoys developing scalable applications.\",\n",
        "    \"The candidate should have experience in data analysis and analyzing large datasets.\",\n",
        "    \"Designing, testing, and maintaining systems is a key responsibility.\"\n",
        "]\n",
        "\n",
        "user_queries = [\n",
        "    \"develop software\",\n",
        "    \"analyze data\",\n",
        "    \"design system\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IREVHGVn45zG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy8U2X6P-LCk"
      },
      "outputs": [],
      "source": [
        "job_descriptions = [\n",
        "    \"The organization was responsible for organizing multiple international conferences.\",\n",
        "    \"Relational databases require normalization and relational mapping techniques.\",\n",
        "    \"The system's conditional logic depends on configurable conditions.\"\n",
        "]\n",
        "\n",
        "user_queries = [\n",
        "    \"organize conference\",\n",
        "    \"relational database\",\n",
        "    \"conditional system\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_i1p0x-7s0s"
      },
      "outputs": [],
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower() # To convert into lowercase characters\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) # To Remove Punctuations\n",
        "    tokens = nltk.word_tokenize(text) # Tokenizer\n",
        "    return tokens\n",
        "\n",
        "# string.punctuation is a constant string containing all ASCII punctuation characters:!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ Contains 32 characters\n",
        "\n",
        "# str.maketrans() creates a translation table — a mapping used by translate().\n",
        "# str.maketrans(x, y, z)\n",
        "# x-->Characters to Replace\n",
        "# y-->Characters to replace with\n",
        "# z-->Characters to delete\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuWq0Bzx-yJk",
        "outputId": "0820b9d6-dbf1-47f3-8ecf-091bc5165b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello world How are you doing today Im finethanks I dont care\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "text = \"Hello, world! How are you doing today? I'm fine-thanks. I don't care\"\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6q7BFko8QV_",
        "outputId": "905a5c03-f5d8-4e26-dd96-efd315f263c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([['the',\n",
              "   'organization',\n",
              "   'was',\n",
              "   'responsible',\n",
              "   'for',\n",
              "   'organizing',\n",
              "   'multiple',\n",
              "   'international',\n",
              "   'conferences'],\n",
              "  ['relational',\n",
              "   'databases',\n",
              "   'require',\n",
              "   'normalization',\n",
              "   'and',\n",
              "   'relational',\n",
              "   'mapping',\n",
              "   'techniques'],\n",
              "  ['the',\n",
              "   'systems',\n",
              "   'conditional',\n",
              "   'logic',\n",
              "   'depends',\n",
              "   'on',\n",
              "   'configurable',\n",
              "   'conditions']],\n",
              " [['organize', 'conference'],\n",
              "  ['relational', 'database'],\n",
              "  ['conditional', 'system']])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preprocess Job Description and User Queries\n",
        "\n",
        "processed_jobs = [preprocess_text(desc) for desc in job_descriptions]\n",
        "processed_queries = [preprocess_text(query) for query in user_queries]\n",
        "\n",
        "processed_jobs, processed_queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxwIYhc98Xr4"
      },
      "outputs": [],
      "source": [
        "# Initialize Stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL996TEC8d-Q",
        "outputId": "7495f15a-edcd-42e8-b0a0-d266dbe2a642"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([['the',\n",
              "   'organ',\n",
              "   'wa',\n",
              "   'respons',\n",
              "   'for',\n",
              "   'organ',\n",
              "   'multipl',\n",
              "   'intern',\n",
              "   'confer'],\n",
              "  ['relat', 'databas', 'requir', 'normal', 'and', 'relat', 'map', 'techniqu'],\n",
              "  ['the', 'system', 'condit', 'logic', 'depend', 'on', 'configur', 'condit']],\n",
              " [['organ', 'confer'], ['relat', 'databas'], ['condit', 'system']])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply porter Stemmer\n",
        "porter_stemmed_jobs = [\n",
        "    [porter_stemmer.stem(word) for word in desc]\n",
        "    for desc in processed_jobs\n",
        "]\n",
        "\n",
        "porter_stemmed_queries = [\n",
        "    [porter_stemmer.stem(word) for word in query]\n",
        "    for query in processed_queries\n",
        "]\n",
        "\n",
        "porter_stemmed_jobs, porter_stemmed_queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5naMLEHk8iaI",
        "outputId": "2d6c070a-355c-45bf-d0b4-90e971045ac3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([['the',\n",
              "   'organ',\n",
              "   'was',\n",
              "   'respons',\n",
              "   'for',\n",
              "   'organ',\n",
              "   'multipl',\n",
              "   'intern',\n",
              "   'confer'],\n",
              "  ['relat', 'databas', 'requir', 'normal', 'and', 'relat', 'map', 'techniqu'],\n",
              "  ['the', 'system', 'condit', 'logic', 'depend', 'on', 'configur', 'condit']],\n",
              " [['organ', 'confer'], ['relat', 'databas'], ['condit', 'system']])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply Snowball Stemmer\n",
        "snowball_stemmed_jobs = [\n",
        "    [snowball_stemmer.stem(word) for word in desc]\n",
        "    for desc in processed_jobs\n",
        "]\n",
        "\n",
        "snowball_stemmed_queries = [\n",
        "    [snowball_stemmer.stem(word) for word in query]\n",
        "    for query in processed_queries\n",
        "]\n",
        "\n",
        "snowball_stemmed_jobs, snowball_stemmed_queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEu3gTZL8o3R",
        "outputId": "d5dae5af-a57c-41df-9a84-e3f3a39ce14d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Job Description Statement 1\n",
            "Original Tokens: ['the', 'organization', 'was', 'responsible', 'for', 'organizing', 'multiple', 'international', 'conferences']\n",
            "Porter Stemmer: ['the', 'organ', 'wa', 'respons', 'for', 'organ', 'multipl', 'intern', 'confer']\n",
            "Snowball Stemmer: ['the', 'organ', 'was', 'respons', 'for', 'organ', 'multipl', 'intern', 'confer']\n",
            "\n",
            "Job Description Statement 2\n",
            "Original Tokens: ['relational', 'databases', 'require', 'normalization', 'and', 'relational', 'mapping', 'techniques']\n",
            "Porter Stemmer: ['relat', 'databas', 'requir', 'normal', 'and', 'relat', 'map', 'techniqu']\n",
            "Snowball Stemmer: ['relat', 'databas', 'requir', 'normal', 'and', 'relat', 'map', 'techniqu']\n",
            "\n",
            "Job Description Statement 3\n",
            "Original Tokens: ['the', 'systems', 'conditional', 'logic', 'depends', 'on', 'configurable', 'conditions']\n",
            "Porter Stemmer: ['the', 'system', 'condit', 'logic', 'depend', 'on', 'configur', 'condit']\n",
            "Snowball Stemmer: ['the', 'system', 'condit', 'logic', 'depend', 'on', 'configur', 'condit']\n"
          ]
        }
      ],
      "source": [
        "# Compare Stemming Results\n",
        "for i in range(len(processed_jobs)):\n",
        "    print(f\"\\nJob Description Statement {i+1}\")\n",
        "    print(\"Original Tokens:\", processed_jobs[i])\n",
        "    print(\"Porter Stemmer:\", porter_stemmed_jobs[i])\n",
        "    print(\"Snowball Stemmer:\", snowball_stemmed_jobs[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK71VQH5Erf0"
      },
      "source": [
        "## Lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NBZMVg1-8y6"
      },
      "outputs": [],
      "source": [
        "# Import Required Libraries.\n",
        "import nltk\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMA_MhUhE0Ay",
        "outputId": "099ea821-3f3c-44f5-84e1-e7c0eb61add6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download required NLTK Resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # OMW = Open Multilingual WordNet, Version 1.4 is a large, curated dataset that links WordNet concepts across multiple languages.\n",
        "nltk.download('averaged_perceptron_tagger') # Part-of-Speech (POS) tagging model called Averaged Perceptron Tagger.\n",
        "nltk.download('averaged_perceptron_tagger_eng') #English-specific version of NLTK’s Averaged Perceptron Part-of-Speech tagger.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5zLjkAkFAnZ"
      },
      "outputs": [],
      "source": [
        "# Sample Support Ticket Data\n",
        "support_tickets = [\n",
        "    \"Customers were running into issues while installing the application.\",\n",
        "    \"The system crashes when users are logging in repeatedly.\",\n",
        "    \"Files were deleted accidentally and could not be recovered.\",\n",
        "    \"The devices are connected but not responding properly.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfoH8FIkH21P"
      },
      "outputs": [],
      "source": [
        "support_tickets = [\n",
        "    \"Users were running multiple processes in the background.\",\n",
        "    \"Files were deleted accidentally by the system.\",\n",
        "    \"The application crashes when customers are logging in.\",\n",
        "    \"Devices were connected but stopped responding suddenly.\",\n",
        "    \"The system configurations were changed without permission.\",\n",
        "    \"Customers complained that the services were not working properly.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUuQOy9RFGTa"
      },
      "outputs": [],
      "source": [
        "# Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqo_ENJSFKa7",
        "outputId": "32fa6a13-201a-4a6a-d924-1524411a13c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['users',\n",
              "  'were',\n",
              "  'running',\n",
              "  'multiple',\n",
              "  'processes',\n",
              "  'in',\n",
              "  'the',\n",
              "  'background'],\n",
              " ['files', 'were', 'deleted', 'accidentally', 'by', 'the', 'system'],\n",
              " ['the',\n",
              "  'application',\n",
              "  'crashes',\n",
              "  'when',\n",
              "  'customers',\n",
              "  'are',\n",
              "  'logging',\n",
              "  'in'],\n",
              " ['devices', 'were', 'connected', 'but', 'stopped', 'responding', 'suddenly'],\n",
              " ['the',\n",
              "  'system',\n",
              "  'configurations',\n",
              "  'were',\n",
              "  'changed',\n",
              "  'without',\n",
              "  'permission'],\n",
              " ['customers',\n",
              "  'complained',\n",
              "  'that',\n",
              "  'the',\n",
              "  'services',\n",
              "  'were',\n",
              "  'not',\n",
              "  'working',\n",
              "  'properly']]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preprocess Support Tickets\n",
        "processed_tickets = [preprocess_text(ticket) for ticket in support_tickets]\n",
        "processed_tickets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfakL6rVFQSs"
      },
      "outputs": [],
      "source": [
        "# Intialize WordLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oBQf5c-FWME",
        "outputId": "aff5b631-f7e4-4068-e838-2b94c347684e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['user', 'were', 'running', 'multiple', 'process', 'in', 'the', 'background'],\n",
              " ['file', 'were', 'deleted', 'accidentally', 'by', 'the', 'system'],\n",
              " ['the', 'application', 'crash', 'when', 'customer', 'are', 'logging', 'in'],\n",
              " ['device', 'were', 'connected', 'but', 'stopped', 'responding', 'suddenly'],\n",
              " ['the',\n",
              "  'system',\n",
              "  'configuration',\n",
              "  'were',\n",
              "  'changed',\n",
              "  'without',\n",
              "  'permission'],\n",
              " ['customer',\n",
              "  'complained',\n",
              "  'that',\n",
              "  'the',\n",
              "  'service',\n",
              "  'were',\n",
              "  'not',\n",
              "  'working',\n",
              "  'properly']]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Lemmatization without POS Tagging\n",
        "lemmatized_no_pos = [\n",
        "    [lemmatizer.lemmatize(word) for word in ticket]\n",
        "    for ticket in processed_tickets\n",
        "]\n",
        "\n",
        "lemmatized_no_pos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W76l8moFe0O"
      },
      "outputs": [],
      "source": [
        "# POS Tagging Function\n",
        "# Convert NLTK POS tags to WordNet POS tags.\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t30NytBiGIwi",
        "outputId": "800a3cec-8d13-4d1f-e5c6-61414e6288cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['user', 'be', 'run', 'multiple', 'process', 'in', 'the', 'background'],\n",
              " ['file', 'be', 'delete', 'accidentally', 'by', 'the', 'system'],\n",
              " ['the', 'application', 'crash', 'when', 'customer', 'be', 'log', 'in'],\n",
              " ['device', 'be', 'connect', 'but', 'stop', 'respond', 'suddenly'],\n",
              " ['the', 'system', 'configuration', 'be', 'change', 'without', 'permission'],\n",
              " ['customer',\n",
              "  'complain',\n",
              "  'that',\n",
              "  'the',\n",
              "  'service',\n",
              "  'be',\n",
              "  'not',\n",
              "  'work',\n",
              "  'properly']]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Lemmatization with POS Tagging\n",
        "lemmatized_with_pos = []\n",
        "\n",
        "for ticket in processed_tickets:\n",
        "    pos_tags = nltk.pos_tag(ticket)\n",
        "    lemmas = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in pos_tags\n",
        "    ]\n",
        "    lemmatized_with_pos.append(lemmas)\n",
        "\n",
        "lemmatized_with_pos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dPyEnKtGNLt",
        "outputId": "e2da239e-646d-4571-b5e9-9f9600a76a77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Support Ticket 1\n",
            "Original Tokens:       ['users', 'were', 'running', 'multiple', 'processes', 'in', 'the', 'background']\n",
            "Lemmatized (No POS):   ['user', 'were', 'running', 'multiple', 'process', 'in', 'the', 'background']\n",
            "Lemmatized (With POS): ['user', 'be', 'run', 'multiple', 'process', 'in', 'the', 'background']\n",
            "\n",
            "Support Ticket 2\n",
            "Original Tokens:       ['files', 'were', 'deleted', 'accidentally', 'by', 'the', 'system']\n",
            "Lemmatized (No POS):   ['file', 'were', 'deleted', 'accidentally', 'by', 'the', 'system']\n",
            "Lemmatized (With POS): ['file', 'be', 'delete', 'accidentally', 'by', 'the', 'system']\n",
            "\n",
            "Support Ticket 3\n",
            "Original Tokens:       ['the', 'application', 'crashes', 'when', 'customers', 'are', 'logging', 'in']\n",
            "Lemmatized (No POS):   ['the', 'application', 'crash', 'when', 'customer', 'are', 'logging', 'in']\n",
            "Lemmatized (With POS): ['the', 'application', 'crash', 'when', 'customer', 'be', 'log', 'in']\n",
            "\n",
            "Support Ticket 4\n",
            "Original Tokens:       ['devices', 'were', 'connected', 'but', 'stopped', 'responding', 'suddenly']\n",
            "Lemmatized (No POS):   ['device', 'were', 'connected', 'but', 'stopped', 'responding', 'suddenly']\n",
            "Lemmatized (With POS): ['device', 'be', 'connect', 'but', 'stop', 'respond', 'suddenly']\n",
            "\n",
            "Support Ticket 5\n",
            "Original Tokens:       ['the', 'system', 'configurations', 'were', 'changed', 'without', 'permission']\n",
            "Lemmatized (No POS):   ['the', 'system', 'configuration', 'were', 'changed', 'without', 'permission']\n",
            "Lemmatized (With POS): ['the', 'system', 'configuration', 'be', 'change', 'without', 'permission']\n",
            "\n",
            "Support Ticket 6\n",
            "Original Tokens:       ['customers', 'complained', 'that', 'the', 'services', 'were', 'not', 'working', 'properly']\n",
            "Lemmatized (No POS):   ['customer', 'complained', 'that', 'the', 'service', 'were', 'not', 'working', 'properly']\n",
            "Lemmatized (With POS): ['customer', 'complain', 'that', 'the', 'service', 'be', 'not', 'work', 'properly']\n"
          ]
        }
      ],
      "source": [
        "# Side by Side Comparison\n",
        "for i in range(len(processed_tickets)):\n",
        "    print(f\"\\nSupport Ticket {i+1}\")\n",
        "    print(\"Original Tokens:      \", processed_tickets[i])\n",
        "    print(\"Lemmatized (No POS):  \", lemmatized_no_pos[i])\n",
        "    print(\"Lemmatized (With POS):\", lemmatized_with_pos[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zHqDy3jGylX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
